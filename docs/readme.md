**"dango-dingo"**

####  **`README.md` (√úberarbeitet)**

```markdown
# üèõÔ∏è dango-dingo: Der juristische Pr√§zisions-Recherche-Assistent

**Ein privater, hochspezialisierter Wissensassistent zur Analyse und Abfrage von juristischen Fachtexten. `dango-dingo` ist als transparenter und validierbarer Helfer f√ºr das wissenschaftliche juristische Arbeiten konzipiert.**

---

## üß≠ Projektphilosophie

`dango-dingo` folgt strikten Leitprinzipien, die seine Funktion und seinen Zweck definieren:

1.  **Korrektheit vor Kreativit√§t:** Jede Aussage ist direkt auf die zugrundeliegende, kuratierte Wissensbasis zur√ºckf√ºhrbar. Das System erfindet keine Informationen.
2.  **Transparenz als Vertrauensgrundlage:** Der Weg von der Nutzeranfrage zur finalen Antwort ist vollst√§ndig nachvollziehbar. Alle Retrieval- und Analyseschritte sind einsehbar.
3.  **Assistent, nicht Autor:** Die Software ist ein Werkzeug zur Beschleunigung und Absicherung der Recherche, nicht zur Erstellung eigener juristischer Meinungen. Die Verantwortung verbleibt beim Nutzer.
4.  **Objektivit√§t und Neutralit√§t:** Das System ist darauf ausgelegt, juristische Meinungsstreite (z.B. h.M. vs. Mindermeinung) neutral und gewichtet darzustellen, basierend auf den Quellen.

## ‚ú® Kern-Features

- **Multi-Stage-Retrieval:** Eine Kombination aus semantischer, Keyword- und Entit√§ten-basierter Suche f√ºr h√∂chste Relevanz.
- **Answer Quality & Confidence Scoring:** Jede Antwort wird automatisch auf ihre Verl√§sslichkeit, Konsistenz und Quellenabdeckung bewertet.
- **Explainable AI (XAI) Dashboard:** Ein Dashboard zur Visualisierung des gesamten RAG-Prozesses, von der Query-Analyse bis zur Antwortsynthese.
- **Zentrale Wissensbasis:** Synchronisation der Datenbanken (FAISS & SQLite) √ºber ein lokales NAS f√ºr die Nutzung durch mehrere Personen im selben Netzwerk.
- **Native Desktop-Anwendung:** Eine performante und sichere Anwendung, gebaut mit Tauri, die lokal auf dem Client-Rechner l√§uft.
- **Intelligente Zitations-Engine:** Automatische Generierung von korrekten Quellenangaben und Validierung von Rechtsnormen.

## üöÄ Tech-Stack

- **Backend:** Python 3.10+, FastAPI (als lokaler Sidecar-Prozess)
- **Frontend:** Tauri, TypeScript, React (oder ein alternatives modernes Framework)
- **Vektordatenbank:** FAISS
- **Metadatenbank:** SQLite
- **Sprachmodell (LLM):** Ollama (lokal ausgef√ºhrt)
- **KI-Framework:** LangChain

## üõ†Ô∏è Quick Start: Setup & Installation

Eine detaillierte Anleitung f√ºr Entwickler findet sich in [`CONTRIBUTING.md`](./CONTRIBUTING.md).

1.  **Voraussetzungen pr√ºfen:** Python, Node.js, Rust und Ollama m√ºssen installiert sein.
2.  **Repository klonen:** `git clone [URL_IHRES_REPOSITORIES]`
3.  **Backend aufsetzen:** `cd dango-dingo-backend && pip install -r requirements.txt`
4.  **Frontend aufsetzen:** `cd ../dango-dingo-frontend && npm install`
5.  **Datenbank initialisieren:** Konfigurieren Sie den NAS-Pfad und f√ºhren Sie das Ingest-Skript aus.
6.  **Anwendung starten:** `npm run tauri dev`

## üìÑ Architektur-√úberblick

Eine detaillierte Beschreibung und ein Diagramm der Systemarchitektur finden Sie in [`docs/architecture.md`](./docs/architecture.md).

## ü§ù Mitwirken

Richtlinien f√ºr die Weiterentwicklung von `dango-dingo` sind in [`CONTRIBUTING.md`](./CONTRIBUTING.md) dokumentiert.

## üìù Lizenz

[TODO: F√ºgen Sie hier eine Lizenz hinzu, z.B. "Privat, keine Weitergabe".]
```

---

#### **2. `docs/architecture.md` (√úberarbeitet)**

```markdown
# üèõÔ∏è Systemarchitektur: dango-dingo

Dieses Dokument beschreibt die Architektur und den Datenfluss des `dango-dingo` Systems. Die Architektur ist als **"Local First" mit zentraler Datenhaltung** konzipiert.

## 1. Komponenten-Diagramm

Das System besteht aus den folgenden Hauptkomponenten, die lokal auf dem Client-Rechner ausgef√ºhrt werden, aber auf eine zentrale Datenquelle zugreifen.

```mermaid
graph TD
    subgraph "Client-Rechner (macOS/Windows)"
        direction LR
        subgraph "Tauri Desktop-Anwendung (dango-dingo)"
            A[Frontend (React/TS)]
        end
        subgraph "Lokale Prozesse"
            B[Backend (Python/FastAPI Sidecar)]
            E[Ollama LLM]
        end
    end

    subgraph "Heimnetzwerk"
        NAS[NAS / Zentraler Speicher]
    end

    subgraph "Zentrale Wissensbasis auf NAS"
        C[FAISS Vektordatenbank]
        D[SQLite Metadatenbank]
        F[PDF-Originale]
    end

    A -- "1. User-Anfrage (HTTP)" --> B
    B -- "2. Vektorsuche" --> C
    C -- "3. Vektor-IDs" --> B
    B -- "4. Metadaten-Abfrage" --> D
    D -- "5. Text-Chunks & Metadaten" --> B
    B -- "6. LLM-Anfrage" --> E
    E -- "7. Generierte Antwort" --> B
    B -- "8. Finale Antwort (HTTP)" --> A
  
    B -- "Lese-/Schreibzugriff" --> NAS
    NAS -- "enth√§lt" --> C
    NAS -- "enth√§lt" --> D
    NAS -- "enth√§lt" --> F

    style A fill:#cde4ff
    style B fill:#d5e8d4
    style E fill:#e1d5e7
    style NAS fill:#f8cecc
```

## 2. Detaillierter Datenfluss einer Anfrage

1. **User-Anfrage:** Der Nutzer gibt eine Frage in die **`dango-dingo` Tauri-App (A)** ein. Das in TypeScript geschriebene Frontend sendet die Anfrage als HTTP-Request an den lokalen FastAPI-Server.
2. **Backend-Verarbeitung:** Das **FastAPI-Backend (B)**, das als Sidecar-Prozess von Tauri gestartet wurde, empf√§ngt die Anfrage. Es nutzt die KI-Kernmodule (z.B. `QueryEnhancer`), um die Anfrage zu analysieren.
3. **Vektorsuche:** Das Backend erstellt ein Embedding f√ºr die (verbesserte) Anfrage und f√ºhrt eine √Ñhnlichkeitssuche in der **FAISS-Vektordatenbank (C)** durch. Diese liegt physisch auf dem NAS und wird √ºber das Netzwerk angesprochen. FAISS gibt eine Liste der relevantesten Vektor-IDs zur√ºck.
4. **Metadaten-Retrieval:** Mit den Vektor-IDs fragt das Backend die **SQLite-Datenbank (D)** auf dem NAS ab, um die zugeh√∂rigen Volltexte der Chunks sowie deren Metadaten (Buch, Seite, etc.) zu erhalten.
5. **Kontext-Erstellung & LLM-Aufruf:** Das Backend assembliert die erhaltenen Texte zu einem Kontext und formuliert einen pr√§zisen Prompt, der an das lokal laufende **Ollama LLM (E)** gesendet wird.
6. **Antwort-Synthese & Qualit√§tspr√ºfung:** Die Roh-Antwort des LLM wird vom Backend entgegengenommen. Das `AnswerQualitySystem` pr√ºft die Antwort, berechnet einen Confidence Score und formatiert das Ergebnis.
7. **R√ºckgabe an Frontend:** Die finale, strukturierte Antwort wird an das **Tauri-Frontend (A)** zur√ºckgesendet und dem Nutzer angezeigt.

## 3. Ingest-Prozess (Datenaufnahme)

1. Ein neues PDF-Dokument wird im Verzeichnis f√ºr **Originale (F)** auf dem NAS abgelegt.
2. Ein separates Skript (`jura_ki/utils/data_processing.py`) wird manuell ausgef√ºhrt.
3. Das Skript liest das PDF, zerlegt es in Text-Chunks und f√ºgt die Buch-Metadaten zur **SQLite-DB (D)** hinzu.
4. F√ºr jeden Chunk wird ein Embedding berechnet und zusammen mit einer ID in der **FAISS-DB (C)** gespeichert.

Diese entkoppelte Architektur gew√§hrleistet, dass die rechenintensiven Aufgaben lokal ausgef√ºhrt werden, w√§hrend die Daten zentral und f√ºr beide Nutzer synchron gehalten werden.

```

---

Die `CONTRIBUTING.md`, die ADRs und die anderen Dokumente k√∂nnen ebenfalls entsprechend angepasst werden, indem "Jura-KI" oder "das Projekt" durch "dango-dingo" ersetzt wird.

Sehr gut. Mit einem konkreten Namen f√ºhlt sich das Projekt sofort greifbarer an.

**Sollen wir jetzt mit der `CONTRIBUTING.md` (dem Entwickler-Guide) fortfahren?** Dieses Dokument ist entscheidend, damit Sie und Ihre Schwester eine konsistente und reibungslose Entwicklungsumgebung haben.
```
